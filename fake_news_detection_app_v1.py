# -*- coding: utf-8 -*-
"""fake_news_detection_app_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQD82fYRQmCwhZQQO0U8JCwNL5za9IRO
"""
import openai

print(openai.__version__)

# Initiate OpenAI
import os
api_key = os.getenv("OPENAI_API_KEY")
openai.api_key = api_key

# Check if API key is being loaded correctly
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    raise ValueError("API Key is not set. Please check your secrets configuration.")

# Test with request
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",  # Or use gpt-4 if you have access
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello! How can I help detect fake news?"}
    ],
    max_tokens=50
)
print(response.choices[0].text)

# Make a request to the Chat API
try:
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  # Use "gpt-4" if needed
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello! How can I help detect fake news?"}
        ],
        max_tokens=50
    )
    print(response.choices[0].message.content)
except Exception as e:
    print(f"Error: {e}")

import requests
from bs4 import BeautifulSoup

def fetch_article_content(url):
    try:
        # Add headers to mimic a browser
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        # Fetch the HTML content of the page
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise error if the request fails

        # Parse the HTML using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract the main content (adjust selectors based on website structure)
        paragraphs = soup.find_all('p')
        article_text = " ".join([p.get_text() for p in paragraphs])

        return article_text
    except Exception as e:
        print(f"Error fetching the article: {e}")
        return None

# Example usage:
url = input("Enter the article URL: ")
article_text = fetch_article_content(url)
if article_text:
    print("\nExtracted Article Text:")
    print(article_text[:500])  # Print only the first 500 characters for preview

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_word_cloud(text):
    # Create a word cloud object
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        colormap='viridis',  # Optional: set a color theme
        stopwords=None       # You can add a list of words to ignore
    ).generate(text)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")  # Remove axes
    plt.title("Word Cloud of Article", fontsize=16)
    plt.show()

if article_text:
    generate_word_cloud(article_text)

def summarize_text(text, max_length=100):
    try:
        # Make a request to GPT for summarization
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # Use "gpt-4" if available
            messages=[
                {"role": "system", "content": "You are an assistant summarizing articles."},
                {"role": "user", "content": f"Summarize the following article in under {max_length} words:\n{text}"}
            ],
            max_tokens=150
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error summarizing the text: {e}")
        return None

if article_text:
    print("\nGenerating Summary...")
    summary = summarize_text(article_text, max_length=100)
    if summary:
        print("\nArticle Summary:")
        print(summary)

"""Data Time"""

import pandas as pd

# Loading the datasets
fake_df = pd.read_csv("Fake.csv", on_bad_lines='skip')
true_df = pd.read_csv("True.csv", on_bad_lines='skip')

true_df.shape

fake_df.shape

# Add a label column: 1 for fake, 0 for real
fake_df["label"] = 1
true_df["label"] = 0

# Combine the datasets
df = pd.concat([fake_df, true_df], axis=0).reset_index(drop=True)

# Shuffle the dataset for randomness
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Preview the combined dataset
print(df.head())

df.shape

# Keep only the relevant columns
df = df[["text", "label"]]

# Drop rows with missing text
df = df.dropna()

df.shape

"""Preprocessing & Vectorizing Text"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initializing TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")

# Transforming the text into feature vectors
X = vectorizer.fit_transform(df["text"])
y = df["label"]  # Labels: 1 (Fake), 0 (True)

X.shape



y.shape

"""Splitting the dataset"""

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape

X_test.shape

"""Training a Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression

# Train LR model
model = LogisticRegression(max_iter=1000)  # Increase max_iter for convergence
model.fit(X_train, y_train)

print("LR Model training complete!")

import pickle

# Save the trained model and vectorizer again
with open("fake_news_model.pkl", "wb") as model_file:
    pickle.dump(model, model_file)

with open("vectorizer.pkl", "wb") as vectorizer_file:
    pickle.dump(vectorizer, vectorizer_file)

print("Model and vectorizer saved successfully!")

"""Evaluate Logistic Regression Model"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Predict on the testing set
y_pred = model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%\n")

# Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=["True News", "Fake News"]))

# Confusion Matrix
print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

"""Saving the Logistic Regression Model & Vectorizer"""

import joblib

joblib.dump(model, "fake_news_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

print("Logistic Regression model and vectorizer saved successfully!")

"""Predict Fake News on New Data with Logistic Regression Model"""

# Load the saved LR model and vectorizer
model = joblib.load("fake_news_model.pkl")
vectorizer = joblib.load("vectorizer.pkl")

def predict_fake_news(text):
    # Transform the input text into feature vectors
    features = vectorizer.transform([text])
    # Predict the probability of being fake
    probability = model.predict_proba(features)[0][1]  # Probability of being fake
    return probability

sample_text = "This is a sample news article to test the model."
fake_probability = predict_fake_news(sample_text)
print(f"\nPrediction: {fake_probability * 100:.2f}% chance of being fake.")

"""Creating Pipeline"""

import requests
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import joblib
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# 1. Fetch Article Content
def fetch_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        article_text = " ".join([p.get_text() for p in paragraphs])
        return article_text
    except Exception as e:
        print(f"Error fetching article: {e}")
        return None

# 2. Generate Word Cloud
def generate_word_cloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title("Word Cloud of Article")
    plt.show()

# 3. Summarize Text using OpenAI
def summarize_text(text, max_length=100):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"Summarize this article in under {max_length} words:\n{text}"}
            ],
            max_tokens=150
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error summarizing text: {e}")
        return None

# 4. Predict Fake News
def predict_fake_news(text):
    model = joblib.load("fake_news_model.pkl")
    vectorizer = joblib.load("vectorizer.pkl")
    features = vectorizer.transform([text])
    probability = model.predict_proba(features)[0][1]
    return probability

# 5. End-to-End Pipeline
def fake_news_pipeline(url_or_text):
    if url_or_text.startswith("http"):
        article_text = fetch_article_content(url_or_text)
        if not article_text:
            return
    else:
        article_text = url_or_text

    print("\n--- Step 1: Word Cloud ---")
    generate_word_cloud(article_text)

    print("\n--- Step 2: Article Summary ---")
    summary = summarize_text(article_text)
    if summary:
        print("Summary:\n", summary)

    print("\n--- Step 3: Fake News Prediction ---")
    fake_probability = predict_fake_news(article_text)
    print(f"Prediction: {fake_probability * 100:.2f}% chance of being fake.")

# Sample Test
url_or_text = input("Enter a URL or plain text: ")
fake_news_pipeline(url_or_text)

import os

# Check for saved model and vectorizer
print("Model file exists:", os.path.exists("fake_news_model.pkl"))
print("Vectorizer file exists:", os.path.exists("vectorizer.pkl"))

import pickle

# Load the saved model and vectorizer
with open("fake_news_model.pkl", "rb") as model_file:
    model = pickle.load(model_file)

with open("vectorizer.pkl", "rb") as vectorizer_file:
    vectorizer = pickle.load(vectorizer_file)

# Test the loaded model
sample_text = ["Breaking news: This is a fake news example to test the model."]
sample_vector = vectorizer.transform(sample_text)
prediction = model.predict(sample_vector)

print("Prediction:", "Fake News" if prediction[0] == 1 else "True News")

# Function to predict news authenticity
def predict_news():
    print("\nEnter a news article below (or type 'exit' to stop):\n")
    while True:
        user_input = input("Your News Article: ")
        if user_input.lower() == "exit":
            print("Exiting the prediction...")
            break

        # Preprocess and predict
        input_vector = vectorizer.transform([user_input])
        prediction = model.predict(input_vector)

        # Display prediction
        if prediction[0] == 1:
            print("Prediction: ðŸš¨ Fake News\n")
        else:
            print("Prediction: âœ… True News\n")

# Run the prediction function
predict_news()
